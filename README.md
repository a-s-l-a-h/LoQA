<div align="center">
  
<h1>LoQA</h1>

  
  **Local Question Answer**
  
  *A simple, offline, cross-platform AI chat application*
  

</div>

---

## ğŸ¯ Philosophy

LoQA is built on three core principles that make AI accessible to everyone:

> **ğŸ  Local First** â€¢ All models and conversations are stored and processed on your device. No internet connection required for chatting.

> **ğŸŒ Cross-Platform** â€¢ A single codebase delivers a native experience on both Windows and Android.

> **â™¿ Accessibility** â€¢ Designed to run efficiently on standard consumer hardware, from your PC to your phone.

---

## ğŸš€ See LoQA in Action

<table align="center">
  <tr>
    <td align="center">
      <h3>ğŸ“± Android Demo</h3>
      <video src="https://github.com/user-attachments/assets/6ba8d4dc-4665-4ce0-8831-3dd1cd168759" 
             controls muted autoplay loop style="max-width:100%; border-radius: 8px;">
        Your browser does not support the video tag.
      </video>
    </td>
  </tr>
  <tr>
    <td align="center">
      <h3>ğŸ’» Windows Demo</h3>
      <video src="https://github.com/user-attachments/assets/6d5cb8dc-3a2f-4e30-9c1f-7921dba416f8" 
             controls muted autoplay loop style="max-width:100%; border-radius: 8px;">
        Your browser does not support the video tag.
      </video>
    </td>
  </tr>
</table>

---

## âš¡ Features

<div align="center">

| Feature | Description |
|---------|-------------|
| **ğŸ¦™ llama.cpp Powered** | Supports a wide range of GGUF models with flexible size and capability options |
| **ğŸ”’ Offline Operation** | Your chats and models are processed entirely locally |
| **ğŸ“±ğŸ’» Cross-Platform** | Seamless experience on Windows and Android |
| **ğŸ”§ Model Management** | Easy GGUF file addition, configuration, and model switching |
| **ğŸ’¾ Conversation History** | Automatic chat saving to local SQLite database |
| **ğŸ›ï¸ Parameter Control** | Real-time adjustment of sampling parameters like temperature |
| **âš™ï¸ Advanced Settings** | Customize context size (CTX) and GPU layer allocation |

</div>

---

## ğŸ—ï¸ Built With

<div align="center">

```mermaid
graph TD
    B[.NET MAUI] --> A[LoQA App]
    C[EasyChatEngine] --> A[LoQA App]
    D[llama.cpp] --> C[EasyChatEngine]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
```

</div>

- **ğŸ¨ .NET MAUI** - Cross-platform UI framework
- **âš¡ EasyChatEngine** - Custom C++ wrapper for simplified llama.cpp integration
- **ğŸš€ llama.cpp** - High-performance GGUF model inference engine

---

## ğŸ› ï¸ Build Instructions

### Prerequisites

- ğŸ“¦ .NET 9 SDK or later
- ğŸ¯ Visual Studio 2022 with ".NET Multi-platform App UI development" workload

### Step 1: Build the Native Engine

```bash
# Clone the engine repository
git clone https://github.com/a-s-l-a-h/easychatengine.git

# Follow the build instructions in that repository's README
# to compile native libraries for your target platforms
```

### Step 2: Place Compiled Binaries

Copy the resulting library files to the correct platform folders:

#### Windows (x64)
```
LoQA/Platforms/Windows/libs/x64/
â”œâ”€â”€ easychatengine.dll
â”œâ”€â”€ llama.dll
â””â”€â”€ ... (other .dll files)
```

#### Android (arm64-v8a)
```
LoQA/Platforms/Android/libs/arm64-v8a/
â”œâ”€â”€ libeasychatengine.so
â”œâ”€â”€ libllama.so
â””â”€â”€ ... (other .so files)
```

### Step 3: Build LoQA

1. Open `LoQA.sln` in Visual Studio 2022
2. Select your target platform (Windows Machine or Android device)
3. Build and run the project

---

## ğŸ“– Quick Start Guide

<div align="center">

### ğŸ¯ Get Started in 5 Easy Steps

</div>

| Step | Action | Details |
|------|--------|---------|
| **1ï¸âƒ£** | **Download Model** | Get a GGUF model from [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&library=gguf&apps=llama.cpp&sort=trending) |
| **2ï¸âƒ£** | **Launch LoQA** | Start the application on your device |
| **3ï¸âƒ£** | **Add Model** | Sidebar â†’ **Models** â†’ **+ Add Model** â†’ Select your GGUF file |
| **4ï¸âƒ£** | **Load Model** | Click the **Load** button next to your model |
| **5ï¸âƒ£** | **Start Chatting** | Click **+ New Chat** and begin your conversation! |

---

## ğŸ“ Project Architecture
```
ğŸ“ LoQA/
â”œâ”€â”€ ğŸ”§ Services/
â”‚ â”œâ”€â”€ EasyChatEngine.cs # C# wrapper for the native C++ engine
â”‚ â”œâ”€â”€ EasyChatService.cs # Core application logic for chat operations
â”‚ â””â”€â”€ DatabaseService.cs # Handles local SQLite database for conversations
â”‚
â”œâ”€â”€ ğŸ¨ Views/
â”‚ â”œâ”€â”€ ChatContentPage.xaml # UI for the main chat interface
â”‚ â”œâ”€â”€ ModelsPage.xaml # UI for adding, configuring, and managing models
â”‚ â””â”€â”€ ... # Other UI pages and controls
â”‚
â””â”€â”€ ğŸ“± Platforms/
â”œâ”€â”€ Windows/libs/x64/
â”‚ â””â”€â”€ ğŸ’» .dll files (e.g., easychatengine.dll, llama.dll)
â”‚
â””â”€â”€ Android/libs/arm64-v8a/
â””â”€â”€ ğŸ“± .so files (e.g., libeasychatengine.so, libllama.so)
```
