# <div align="center">
  <svg width="200" height="100" xmlns="http://www.w3.org/2000/svg">
    <defs>
      <filter id="textShadow" x="-50%" y="-50%" width="200%" height="200%">
        <feDropShadow dx="2" dy="2" stdDeviation="2" flood-color="rgba(255,255,255,0.3)" />
      </filter>
    </defs>
    <rect width="100%" height="100%" fill="#1a1a1a"/>
    <text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle"
          font-family="Arial, sans-serif" font-size="48" font-weight="bold"
          fill="white" filter="url(#textShadow)">
      LoQA
    </text>
  </svg>
  
  **Local Question Answer**
  
  *A simple, offline, cross-platform AI chat application*

</div>

---

## ğŸ¯ Philosophy

LoQA is built on three core principles that make AI accessible to everyone:

> **ğŸ  Local First** â€¢ All models and conversations are stored and processed on your device. No internet connection required for chatting.

> **ğŸŒ Cross-Platform** â€¢ A single codebase delivers a native experience on both Windows and Android.

> **â™¿ Accessibility** â€¢ Designed to run efficiently on standard consumer hardware, from your PC to your phone.

---

## ğŸš€ See LoQA in Action

<table align="center">
  <tr>
    <td align="center" width="50%">
      <h3>ğŸ“± Android Demo</h3>
      <video src="https://github.com/user-attachments/assets/6ba8d4dc-4665-4ce0-8831-3dd1cd168759" 
             controls muted autoplay loop style="max-width:100%; border-radius: 8px;">
        Your browser does not support the video tag.
      </video>
    </td>
    <td align="center" width="50%">
      <h3>ğŸ’» Windows Demo</h3>
      <video src="https://github.com/user-attachments/assets/6d5cb8dc-3a2f-4e30-9c1f-7921dba416f8" 
             controls muted autoplay loop style="max-width:100%; border-radius: 8px;">
        Your browser does not support the video tag.
      </video>
    </td>
  </tr>
</table>

---

## âš¡ Features

<div align="center">

| Feature | Description |
|---------|-------------|
| **ğŸ¦™ llama.cpp Powered** | Supports a wide range of GGUF models with flexible size and capability options |
| **ğŸ”’ Offline Operation** | Your chats and models are processed entirely locally |
| **ğŸ“±ğŸ’» Cross-Platform** | Seamless experience on Windows and Android |
| **ğŸ”§ Model Management** | Easy GGUF file addition, configuration, and model switching |
| **ğŸ’¾ Conversation History** | Automatic chat saving to local SQLite database |
| **ğŸ›ï¸ Parameter Control** | Real-time adjustment of sampling parameters like temperature |
| **âš™ï¸ Advanced Settings** | Customize context size (CTX) and GPU layer allocation |

</div>

---

## ğŸ—ï¸ Built With

<div align="center">

```mermaid
graph TD
    A[LoQA App] --> B[.NET MAUI]
    A --> C[EasyChatEngine]
    C --> D[llama.cpp]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
```

</div>

- **ğŸ¨ .NET MAUI** - Cross-platform UI framework
- **âš¡ EasyChatEngine** - Custom C++ wrapper for simplified llama.cpp integration
- **ğŸš€ llama.cpp** - High-performance GGUF model inference engine

---

## ğŸ› ï¸ Build Instructions

### Prerequisites

- ğŸ“¦ .NET 9 SDK or later
- ğŸ¯ Visual Studio 2022 with ".NET Multi-platform App UI development" workload

### Step 1: Build the Native Engine

```bash
# Clone the engine repository
git clone https://github.com/a-s-l-a-h/easychatengine.git

# Follow the build instructions in that repository's README
# to compile native libraries for your target platforms
```

### Step 2: Place Compiled Binaries

Copy the resulting library files to the correct platform folders:

#### Windows (x64)
```
LoQA/Platforms/Windows/libs/x64/
â”œâ”€â”€ easychatengine.dll
â”œâ”€â”€ llama.dll
â””â”€â”€ ... (other .dll files)
```

#### Android (arm64-v8a)
```
LoQA/Platforms/Android/libs/arm64-v8a/
â”œâ”€â”€ libeasychatengine.so
â”œâ”€â”€ libllama.so
â””â”€â”€ ... (other .so files)
```

### Step 3: Build LoQA

1. Open `LoQA.sln` in Visual Studio 2022
2. Select your target platform (Windows Machine or Android device)
3. Build and run the project

---

## ğŸ“– Quick Start Guide

<div align="center">

### ğŸ¯ Get Started in 5 Easy Steps

</div>

| Step | Action | Details |
|------|--------|---------|
| **1ï¸âƒ£** | **Download Model** | Get a GGUF model from [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&library=gguf&apps=llama.cpp&sort=trending) |
| **2ï¸âƒ£** | **Launch LoQA** | Start the application on your device |
| **3ï¸âƒ£** | **Add Model** | Sidebar â†’ **Models** â†’ **+ Add Model** â†’ Select your GGUF file |
| **4ï¸âƒ£** | **Load Model** | Click the **Load** button next to your model |
| **5ï¸âƒ£** | **Start Chatting** | Click **+ New Chat** and begin your conversation! |

---

## ğŸ“ Project Architecture

<div align="center">

```
LoQA/
â”œâ”€â”€ ğŸ”§ Services/
â”‚   â”œâ”€â”€ EasyChatEngine.cs     # C# wrapper for native backend
â”‚   â”œâ”€â”€ EasyChatService.cs    # Application state & chat logic
â”‚   â””â”€â”€ DatabaseService.cs    # SQLite database management
â”œâ”€â”€ ğŸ¨ Views/
â”‚   â”œâ”€â”€ ChatContentPage.xaml  # Main chat interface
â”‚   â”œâ”€â”€ ModelsPage.xaml       # Model management UI
â”‚   â””â”€â”€ ...                   # Other UI components
â””â”€â”€ ğŸ“± Platforms/
    â”œâ”€â”€ Windows/libs/x64/     # Windows native libraries
    â””â”€â”€ Android/libs/arm64-v8a/ # Android native libraries
```

</div>

---



</div>
